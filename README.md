# DataPulse â€“ Projet Data Warehouse & Analytics 
Bienvenue dans le dÃ©pÃ´t **DataPulse â€“ Projet Data Warehouse & Analytics**.

Ce projet prÃ©sente un **pipeline complet de data engineering de bout en bout**, basÃ© sur une **architecture Lakehouse avec Medallion Architecture (Bronze / Silver / Gold)**. Il a Ã©tÃ© conÃ§u comme un **projet acadÃ©mique et de portfolio**, alignÃ© avec les **bonnes pratiques modernes en data engineering et data analytics**.

<img width="1536" height="1024" alt="architecture_data" src="https://github.com/user-attachments/assets/3da23056-66d9-45a7-ae15-b5cb8301dcd9" />


## ğŸ—ï¸ Architecture des donnÃ©es

Lâ€™architecture de DataPulse repose sur le modÃ¨le **Medallion Architecture** :

* **Couche Bronze** : ingestion des donnÃ©es brutes depuis plusieurs sources
* **Couche Silver** : nettoyage, normalisation et mise en conformitÃ© RGPD
* **Couche Gold** : donnÃ©es analytiques prÃªtes pour la consommation mÃ©tier

Cette architecture garantit :

* la traÃ§abilitÃ© complÃ¨te des transformations,
* la scalabilitÃ©,
* la robustesse,
* une sÃ©paration claire des responsabilitÃ©s.

---

## ğŸ“– PrÃ©sentation du projet

Le projet couvre lâ€™ensemble du cycle de vie de la donnÃ©e :

* **Ingestion** de sources hÃ©tÃ©rogÃ¨nes (web scraping, API REST, fichiers Excel)
* **Stockage des donnÃ©es brutes** dans un stockage objet (Data Lake)
* **Traitements ETL** en Python
* **ModÃ©lisation Data Warehouse** en schÃ©ma en Ã©toile (Star Schema)
* **Analyses SQL** pour la BI et le reporting

ğŸ¯ Ce projet met en valeur les compÃ©tences suivantes :

* Data Engineering
* Data Warehousing
* Conception de pipelines ETL
* SQL & analytique
* ModÃ©lisation de donnÃ©es (Star Schema)

---

## ğŸ› ï¸ Technologies utilisÃ©es

| Couche           | Technologie                          |
| ---------------- | ------------------------------------ |
| Ingestion        | Python (Scraping, API REST, Excel)   |
| Stockage Bronze  | MinIO (Object Storage compatible S3) |
| Transformation   | Python ETL                           |
| Data Warehouse   | PostgreSQL                           |
| Orchestration    | Python CLI                           |
| Analytique       | SQL / pgAdmin                        |
| Conteneurisation | Docker & Docker Compose              |

---

## ğŸ§± ImplÃ©mentation des couches Medallion

### ğŸŸ¤ Couche Bronze â€“ DonnÃ©es brutes

**Objectif :** stocker les donnÃ©es telles quâ€™elles sont reÃ§ues depuis les sources, sans transformation.

**Sources :**

* Web scraping (livres et citations)
* API REST (donnÃ©es gÃ©ographiques)
* Fichiers Excel (partenaires / librairies)

**ImplÃ©mentation :**

* Scripts Python dâ€™ingestion
* Stockage des fichiers bruts dans **MinIO** (JSON / CSV / Excel)

---

### âšª Couche Silver â€“ DonnÃ©es nettoyÃ©es

**Objectif :** prÃ©parer les donnÃ©es pour lâ€™analyse via :

* nettoyage,
* normalisation,
* contrÃ´les qualitÃ©,
* rÃ¨gles de conformitÃ© RGPD.

**ImplÃ©mentation :**

* Logique de transformation en Python
* Chargement des donnÃ©es nettoyÃ©es dans **PostgreSQL**

---

### ğŸŸ¡ Couche Gold â€“ DonnÃ©es prÃªtes mÃ©tier

**Objectif :** fournir des donnÃ©es optimisÃ©es pour lâ€™analyse, le reporting et les outils BI.

**ImplÃ©mentation :**

* ModÃ©lisation en schÃ©ma en Ã©toile (tables de faits et de dimensions)
* Stockage dans PostgreSQL

---

## â­ Couche Gold â€“ ModÃ©lisation des donnÃ©es

La couche Gold implÃ©mente un **Data Mart orientÃ© ventes**, basÃ© sur un **schÃ©ma en Ã©toile**, optimisÃ© pour les requÃªtes analytiques.

### Tables de dimensions

**`gold.dim_books`**

* book_key (PK)
* book_id
* title
* category
* publication_year
* sentiment

**`gold.dim_authors`**

* author_key (PK)
* author_id
* author_name

**`gold.dim_geo`**

* geo_key (PK)
* postal_code
* city
* department
* country

### Table de faits

**`gold.fact_sales`**

* clÃ©s Ã©trangÃ¨res vers les dimensions
* quantity
* price

**Indicateur mÃ©tier :**

```
sales_amount = quantity * price
```

---

## ğŸ“‚ Structure rÃ©elle du projet

ecf_complet/
â”‚
â”œâ”€â”€ docker_compose.yml                # Infrastructure Docker
â”‚
â”œâ”€â”€ docs/                             # Documentation du projet
â”‚   â”œâ”€â”€ architecture_data.png         # Diagramme global dâ€™architecture
â”‚   â”œâ”€â”€ model_data.png                # ModÃ¨le de donnÃ©es (Star Schema)
â”‚   â”œâ”€â”€ dat.md                        # Dictionnaire / documentation des donnÃ©es
â”‚   â””â”€â”€ rgpd_conformite.md            # ConformitÃ© RGPD
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                   # Configuration centralisÃ©e
â”‚
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ minio_client.py               # Client MinIO (Data Lake â€“ Bronze)
â”‚
â”œâ”€â”€ sql/                              # RequÃªtes SQL analytiques
â”‚   â”œâ”€â”€ analyses.sql                  # Analyses SQL PostgreSQL
â”‚   â””â”€â”€ analysis_sql_pandas.sql       # Analyses SQL utilisÃ©es avec Pandas
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py                   # Orchestrateur CLI (Bronze / Silver / Gold)
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/                    # Couche Bronze â€“ ingestion des donnÃ©es
â”‚   â”‚   â”œâ”€â”€ scrape_books.py           # Web scraping livres
â”‚   â”‚   â”œâ”€â”€ scrape_quotes.py          # Web scraping citations
â”‚   â”‚   â”œâ”€â”€ scrape_api_geo.py         # Ingestion API REST
â”‚   â”‚   â””â”€â”€ import_excel.py           # Import fichiers Excel
â”‚   â”‚
â”‚   â””â”€â”€ transformation/               # Transformations Silver & Gold
â”‚       â”œâ”€â”€ bronze_to_silver.py       # Nettoyage & normalisation
â”‚       â””â”€â”€ silver_to_gold.py         # ModÃ©lisation analytique
â”‚
â””â”€â”€ .vscode/
    â””â”€â”€ settings.json                 # Configuration Ã©diteur

---

## âš™ï¸ Guide dâ€™exÃ©cution

### 1ï¸âƒ£ Lancer lâ€™infrastructure

```bash
docker-compose up -d
```

---

### 2ï¸âƒ£ AccÃ©der au conteneur ETL

```bash
docker exec -it datapulse_etl bash
```

---

### 3ï¸âƒ£ Installer les dÃ©pendances Python

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ ExÃ©cuter le pipeline

**Bronze â€“ Ingestion**

```bash
python src/pipeline.py --step bronze
```

**Silver â€“ Nettoyage et normalisation**

```bash
python src/pipeline.py --step silver
```

**Gold â€“ ModÃ©lisation analytique**

```bash
python src/pipeline.py --step gold
```

---

## ğŸ“Š Consommation des donnÃ©es

Les donnÃ©es de la couche Gold peuvent Ãªtre exploitÃ©es via :

* des requÃªtes SQL (PostgreSQL),
* pgAdmin,
* des outils BI (Power BI, Tableau),
* des analyses avancÃ©es et data science.


